from qdrant_client import QdrantClient
from qdrant_client.http import models
from langchain_openai import OpenAIEmbeddings
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from langchain.output_parsers import StructuredOutputParser, ResponseSchema
from langchain.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
import torch
from dotenv import load_dotenv
import os
import uuid
import logging
from functools import lru_cache
from tiktoken import get_encoding

logging.basicConfig(level=logging.INFO)
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")

COLLECTION_NAME = "pdf_docs"
USE_OPENAI = True

# Environment setup
device = "cuda" if torch.cuda.is_available() else "cpu"
dtype = torch.float16 if device == "cuda" else torch.float32

# Connect Qdrant, load embeddings
qdrant = QdrantClient(url="http://localhost:6333")
embeddings = OpenAIEmbeddings(model="text-embedding-3-small", api_key=api_key)

# load reranker
reranker_model_name = "BAAI/bge-reranker-base" #"BAAI/bge-reranker-large"
reranker_tokenizer = AutoTokenizer.from_pretrained(reranker_model_name)
reranker_model = AutoModelForSequenceClassification.from_pretrained(
    reranker_model_name,
    dtype=dtype
).to(device)

st_model = None

def get_or_load_st_model():
    global st_model
    if st_model is None:
        logging.info("üß† Loading SentenceTransformer model...")
        from sentence_transformers import SentenceTransformer
        st_model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
    return st_model

# --- collection management ---
def init_collection(vector_size: int, collection_name="pdf_docs"):
    try:
        qdrant.get_collection(collection_name=collection_name)
    except Exception:
        qdrant.create_collection(
            collection_name=collection_name,
            vectors_config=models.VectorParams(
                size=vector_size,
                distance=models.Distance.COSINE
            ),
        )

# --- ingest ---
def embed_and_store(chunks, payloads=None, collection="pdf_docs"):
    """Embed chunks + store in Qdrant + extract metadata"""
    enc = get_encoding("cl100k_base")
    safe_chunks = []
    max_token_limit = 8000  # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö text-embedding-3-small/large
    max_total_tokens = 250000

    # --- 1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞ split text ‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô limit ---
    for i, c in enumerate(chunks):
        text = c.page_content.strip()
        tokens = len(enc.encode(text))

        if tokens > max_token_limit:
            logging.warning(f"‚ö†Ô∏è Chunk {i} ‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô limit ({tokens} tokens) ‚Üí ‡πÅ‡∏ö‡πà‡∏á‡∏¢‡πà‡∏≠‡∏¢")
            # ‡πÅ‡∏ö‡πà‡∏á text ‡∏¢‡πà‡∏≠‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 7000 tokens
            words = text.split()
            sub_chunk = ""
            sub_tokens = 0
            for w in words:
                t_len = len(enc.encode(w))
                if sub_tokens + t_len > 7000:
                    safe_chunks.append(c.__class__(page_content=sub_chunk.strip()))
                    sub_chunk = ""
                    sub_tokens = 0
                sub_chunk += " " + w
                sub_tokens += t_len
            if sub_chunk.strip():
                safe_chunks.append(c.__class__(page_content=sub_chunk.strip()))
        else:
            safe_chunks.append(c)

    logging.info(f"‚úÖ ‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(safe_chunks)} chunks ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏ï‡∏£‡∏ß‡∏à limit")

    # --- 2. embed ---
    vectors = []
    if USE_OPENAI:
        logging.info("üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings ‡∏î‡πâ‡∏ß‡∏¢ OpenAI ‡πÅ‡∏ö‡∏ö batch...")

        current_batch = []
        current_tokens = 0

        for i, c in enumerate(safe_chunks):
            text = c.page_content.strip()
            tokens = len(enc.encode(text))

            # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏Å‡∏¥‡∏ô limit ‡∏£‡∏ß‡∏° ‚Üí ‡∏™‡πà‡∏á batch ‡∏ô‡∏µ‡πâ‡∏Å‡πà‡∏≠‡∏ô
            if current_tokens + tokens > max_total_tokens:
                logging.info(f"üì¶ ‡∏™‡πà‡∏á batch (‡∏£‡∏ß‡∏° {len(current_batch)} chunks, {current_tokens} tokens)")
                batch_vectors = embeddings.embed_documents(current_batch)
                vectors.extend(batch_vectors)
                current_batch = []
                current_tokens = 0

            current_batch.append(text)
            current_tokens += tokens

        # ‡∏™‡πà‡∏á batch ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
        if current_batch:
            logging.info(f"üì¶ ‡∏™‡πà‡∏á batch ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ (‡∏£‡∏ß‡∏° {len(current_batch)} chunks, {current_tokens} tokens)")
            batch_vectors = embeddings.embed_documents(current_batch)
            vectors.extend(batch_vectors)

        logging.info(f"‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(vectors)} vectors")

    else:
        # ‡∏ñ‡πâ‡∏≤‡πÉ‡∏ä‡πâ SentenceTransformer
        st = get_or_load_st_model()
        vectors = st.encode(
            [c.page_content for c in safe_chunks],
            convert_to_numpy=True,
            show_progress_bar=True
        )

    # --- 3. ‡∏™‡∏£‡πâ‡∏≤‡∏á structured metadata ---
    schemas = [
        ResponseSchema(name="person", description="‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•‡∏´‡∏£‡∏∑‡∏≠‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£"),
        ResponseSchema(name="wealth", description="‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≤‡∏ó‡∏£‡∏±‡∏û‡∏¢‡πå‡∏™‡∏¥‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ"),
        ResponseSchema(name="industry", description="‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏≠‡∏∏‡∏ï‡∏™‡∏≤‡∏´‡∏Å‡∏£‡∏£‡∏°‡∏´‡∏£‡∏∑‡∏≠‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à"),
        ResponseSchema(name="rank", description="‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ç‡∏≠‡∏á‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏• ‡πÄ‡∏ä‡πà‡∏ô ‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö 1, ‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö 2"),
    ]
    parser = StructuredOutputParser.from_response_schemas(schemas)
    format_instructions = parser.get_format_instructions()
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

    prompt_template = ChatPromptTemplate.from_template(
        "Extract person, wealth, and industry from the following text.\n"
        "{format_instructions}\n\nText:\n{input_text}"
    )

    auto_metadata = []
    for i, chunk in enumerate(safe_chunks):
        prompt = prompt_template.format_messages(
            input_text=chunk.page_content,
            format_instructions=format_instructions
        )
        try:
            response = llm.invoke(prompt)
            # parsed = parser.parse(response.content)
            # logging.info(f"üìÑ Parsed metadata chunk {i}: {parsed}")
            raw_output = response.content.strip()
            logging.debug(f"üß© Raw LLM output chunk {i}: {raw_output}")
            parsed = parser.parse(raw_output)
        except Exception as e:
            logging.warning(f"‚ö†Ô∏è Metadata extraction error ‡∏ó‡∏µ‡πà chunk {i}: {e}")
            parsed = {}
        auto_metadata.append(parsed)

    # --- 4. upsert Qdrant ---
    qdrant.upsert(
        collection_name=collection,
        points=[
            models.PointStruct(
                id=str(uuid.uuid4()),
                vector=vectors[i],
                payload={
                    "text": safe_chunks[i].page_content,
                    **(payloads[i] if payloads and i < len(payloads) else {}),
                    **(auto_metadata[i] if i < len(auto_metadata) else {}),
                }
            )
            for i in range(len(safe_chunks))
        ],
    )
    
    logging.info(f"üì¶ Upserted {len(vectors)} points ‚Üí {collection}")
    return {
        "stored": len(vectors),
        "collection": collection,
        "payload_samples": payloads[:2] if payloads else []
    }
# --- search (‡πÄ‡∏Å‡πà‡∏≤) ---
def query_vector_db(query: str, top_k: int = 5):
    if USE_OPENAI:
        vector = embeddings.embed_query(query)
    else:
        vector = get_or_load_st_model().encode(query, convert_to_numpy=True)

    results = qdrant.search(
        collection_name=COLLECTION_NAME,
        query_vector=vector,
        limit=top_k
    )
    return [
        {
            "id": r.id,
            "score": r.score,
            "payload": r.payload
        }
        for r in results
    ]

# --- search (‡πÉ‡∏´‡∏°‡πà) ---
def hybrid_search(query: str, top_k: int = 30, keyword: str = None, collection=COLLECTION_NAME):
    if USE_OPENAI:
        vector = embeddings.embed_query(query)
    else:
        vector = get_or_load_st_model().encode(query, convert_to_numpy=True)

    query_filter = None
    if keyword:
        # üîß Qdrant ‡πÑ‡∏°‡πà‡∏°‡∏µ MatchText ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ MatchValue
        query_filter = models.Filter(
            must=[models.FieldCondition(key="text", match=models.MatchValue(value=keyword))]
        )

    results = qdrant.search(
        collection_name=collection,
        query_vector=vector,
        limit=top_k,
        query_filter=query_filter
    )

    return results

# --- rerank ---
def rerank_results(query: str, results, top_k: int = 5):
    if not results:
        logging.warning("‚ö†Ô∏è No candidates to rerank")
        return []

    # üîß ‡πÉ‡∏ä‡πâ .payload ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á check `"payload" in r`
    pairs = []
    for r in results:
        if not r.payload:
            continue
        doc_text = r.payload.get("text") or r.payload.get("raw")
        if doc_text:
            pairs.append((query, doc_text))

    if not pairs:
        logging.warning("‚ö†Ô∏è No valid text in candidates to rerank")
        return []

    inputs = reranker_tokenizer(
        pairs,
        padding=True,
        truncation=True,
        return_tensors="pt",
        max_length=512
    )

    inputs = {k: v.to(device) for k, v in inputs.items()}
    inputs["input_ids"] = inputs["input_ids"].long()

    with torch.no_grad():
        logits = reranker_model(**inputs).logits

    scores = logits.squeeze(-1).tolist()

    reranked = []
    for r, s in zip(results, scores):
        reranked.append({
            "id": r.id,
            "payload": r.payload,
            "vector_score": r.score,
            "rerank_score": float(s)
        })

    reranked = sorted(reranked, key=lambda x: x["rerank_score"], reverse=True)
    return reranked[:top_k]

# --- hybrid + rerank ---
def query_hybrid_rerank(query: str, keyword: str = None, top_k: int = 5, collection=COLLECTION_NAME):
    candidates = hybrid_search(query, top_k=30, keyword=keyword, collection=collection)

    if not candidates:
        logging.warning(f"‚ö†Ô∏è No candidates found for query='{query}'")
        return []

    reranked = rerank_results(query, candidates, top_k=top_k)
    return reranked
